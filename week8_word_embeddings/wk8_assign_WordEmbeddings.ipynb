{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Word Embeddings \n",
    "\n",
    "Welcome to the fourth (and last) programming assignment of Course 2! \n",
    "\n",
    "In this assignment, you will practice how to compute word embeddings and use them for sentiment analysis.\n",
    "- To implement sentiment analysis, you can go beyond counting the number of positive words and negative words. \n",
    "- You can find a way to represent each word numerically, by a vector. \n",
    "- The vector could then represent syntactic (i.e. parts of speech) and semantic (i.e. meaning) structures. \n",
    "\n",
    "In this assignment, you will explore a classic way of generating word embeddings or representations.\n",
    "- You will implement a famous model called the continuous bag of words (CBOW) model. \n",
    "\n",
    "By completing this assignment you will:\n",
    "\n",
    "- Train word vectors from scratch.\n",
    "- Learn how to create batches of data.\n",
    "- Understand how backpropagation works.\n",
    "- Plot and visualize your learned word vectors.\n",
    "\n",
    "Knowing how to train these models will give you a better understanding of word vectors, which are building blocks to many applications in natural language processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [1 - The Continuous Bag of Words Model](#1)\n",
    "- [2 - Training the Model](#2)\n",
    "    - [2.1 - Initializing the Model](#2.1)\n",
    "        - [Exercise 1 - initialize_model (UNQ_C1)](#ex-1)\n",
    "    - [2.2 - Softmax](#2.2)\n",
    "        - [Exercise 2 - softmax (UNQ_C2)](#ex-2)\n",
    "    - [2.3 - Forward Propagation](#2.3)\n",
    "        - [Exercise 3 - forward_prop (UNQ_C3)](#ex-3)\n",
    "    - [2.4 - Cost Function](#2.4)\n",
    "    - [2.5 - Training the Model - Backpropagation](#2.5)\n",
    "        - [Exercise 4 - back_prop (UNQ_C4)](#ex-4)\n",
    "    - [2.6 - Gradient Descent](#2.6)\n",
    "        - [Exercise 5 - gradient_descent (UNQ_C5)](#ex-5)\n",
    "- [3 - Visualizing the Word Vectors](#3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - The Continuous Bag of Words Model\n",
    "\n",
    "Let's take a look at the following sentence: \n",
    ">**'I am happy because I am learning'**. \n",
    "\n",
    "- In continuous bag of words (CBOW) modeling, we try to predict the center word given a few context words (the words around the center word).\n",
    "- For example, if you were to choose a context half-size of say $C = 2$, then you would try to predict the word **happy** given the context that includes 2 words before and 2 words after the center word:\n",
    "\n",
    "> $C$ words before: [I, am] \n",
    "\n",
    "> $C$ words after: [because, I] \n",
    "\n",
    "- In other words:\n",
    "\n",
    "$$context = [I,am, because, I]$$\n",
    "$$target = happy$$\n",
    "\n",
    "The structure of your model will look like this:\n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/word2.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" /> Figure 1 </div>\n",
    "\n",
    "Where $\\bar x$ is the average of all the one hot vectors of the context words. \n",
    "\n",
    "<div style=\"width:image width px; font-size:100%; text-align:center;\"><img src='images/mean_vec2.png' alt=\"alternate text\" width=\"width\" height=\"height\" style=\"width:600px;height:250px;\" /> Figure 2 </div>\n",
    "\n",
    "Once you have encoded all the context words, you can use $\\bar x$ as the input to your model. \n",
    "\n",
    "The architecture you will be implementing is as follows:\n",
    "\n",
    "\n",
    "$$ h = W_1 \\  X + b_1  \\tag{1} \\\\$$\n",
    " $$a = ReLU(h)  \\tag{2} \\\\$$\n",
    " $$z = W_2 \\  a + b_2   \\tag{3} \\\\$$\n",
    "$$ \\hat y = softmax(z)   \\tag{4} \\\\$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anrui/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Python libraries and helper functions (in utils2) \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from utils2 import sigmoid, get_batches, compute_pca, get_dict\n",
    "import w4_unittest\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
